{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Faster RCNN ResNet50",
   "id": "dde8aa0109ea1b11"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dataset setup",
   "id": "88a9e38de146236a"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-17T21:24:24.435352Z",
     "start_time": "2025-08-17T21:24:22.164889Z"
    }
   },
   "source": [
    "import albumentations.pytorch\n",
    "import datasets as ds\n",
    "import utils\n",
    "import torch as t\n",
    "import torchvision as tv\n",
    "import torchvision.tv_tensors as tvt\n",
    "import albumentations as A\n",
    "\n",
    "# utils.set_seed(42)\n",
    "\n",
    "x, y = ds.extract_all('./datasets/090/annotations.xml')\n",
    "x1, y1 = ds.extract_all('./datasets/190/annotations.xml')\n",
    "x2, y2 = ds.extract_all('./datasets/30/annotations.xml')\n",
    "x3, y3 = ds.extract_all('./datasets/60/annotations.xml')\n",
    "x4, y4 = ds.extract_all('./datasets/90/annotations.xml')\n",
    "\n",
    "x5, y5 = ds.extract_all('./datasets/clear1/annotations.xml')\n",
    "\n",
    "train_ds = ds.CustomImageDataset([*x, *x1, *x2, *x3, *x4], [*y, *y1, *y2, *y3, *y4], transform=A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
    "    A.Affine(\n",
    "        translate_percent={\"x\": 0.0625, \"y\": 0.0625},\n",
    "        scale=(0.9, 1.1),\n",
    "        rotate=(-15, 15),\n",
    "        p=0.5\n",
    "    ),\n",
    "    albumentations.pytorch.ToTensorV2()\n",
    "],\n",
    "    bbox_params=ds.albumentations_params)\n",
    "                                 )\n",
    "\n",
    "val_ds = ds.CustomImageDataset(x5, y5)\n",
    "\n",
    "dl_train = t.utils.data.DataLoader(train_ds, batch_size=4, shuffle=True, collate_fn=utils.unroller)\n",
    "dl_val = t.utils.data.DataLoader(val_ds, batch_size=4, shuffle=True, collate_fn=utils.unroller)\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model setup",
   "id": "9b692c08f0a86915"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T21:24:25.988287Z",
     "start_time": "2025-08-17T21:24:25.605405Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "device = t.device('cuda') if t.cuda.is_available() else t.device('cpu')\n",
    "\n",
    "model = tv.models.detection.fasterrcnn_resnet50_fpn(weights='COCO_V1', backbone_weights='IMAGENET1K_V2').to(device)\n",
    "\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = tv.models.detection.faster_rcnn.FastRCNNPredictor(\n",
    "    in_channels=model.roi_heads.box_predictor.cls_score.in_features,\n",
    "    num_classes=2\n",
    ").to(device)\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = t.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "lr_scheduler = t.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n"
   ],
   "id": "22669c8dbcfd5dc5",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training",
   "id": "dadd172876631816"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T21:25:55.113543Z",
     "start_time": "2025-08-17T21:24:27.302499Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#from tqdm.notebook import tqdm\n",
    "from tqdm.auto import tqdm\n",
    "import torchmetrics as tm\n",
    "\n",
    "# epoch metrics\n",
    "best_val_loss = float('inf')\n",
    "best_val_map = 0.0\n",
    "metric = tm.detection.mean_ap.MeanAveragePrecision(\"xyxy\")\n",
    "\n",
    "# early stopping\n",
    "patience = 5\n",
    "counter = 0\n",
    "best_model_state = None\n",
    "\n",
    "# hyperparams\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # ---- TRAIN ----\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    dl_train_tqdm = tqdm(dl_train, desc=f\"Train Epoch {epoch + 1}\", leave=True)\n",
    "\n",
    "    for images, targets in dl_train_tqdm:\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        total_loss = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += total_loss.item()\n",
    "\n",
    "        dl_train_tqdm.set_postfix(loss=total_loss.item())\n",
    "\n",
    "    train_loss /= len(dl_train)\n",
    "\n",
    "    # ---- VALIDATION ----\n",
    "    val_loss = 0\n",
    "    metric.reset()\n",
    "\n",
    "    with t.no_grad():\n",
    "        for images, targets in dl_val:\n",
    "            images = [img.to(device) for img in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            # loss\n",
    "            #print(targets)\n",
    "            loss_dict = model(images, targets)\n",
    "            #print(loss_dict)\n",
    "\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            val_loss += losses\n",
    "\n",
    "            # predictions\n",
    "\n",
    "            model.eval()\n",
    "            pred = model(images)\n",
    "            metric.update(pred, targets)\n",
    "\n",
    "    val_loss /= len(dl_val)  # avg loss\n",
    "\n",
    "    val_metrics = metric.compute()\n",
    "    val_map = val_metrics[\"map\"].item()\n",
    "\n",
    "    print(f\"epoch={epoch + 1}; train_loss={train_loss:.4f}; val_loss={val_loss:.4f}; val_map={val_map:.4f}\")\n",
    "\n",
    "    # ---- Early Stopping ----\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "\n",
    "        best_model_state = model.state_dict()\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            model.load_state_dict(best_model_state)\n",
    "            break\n"
   ],
   "id": "a83e29ab00cfc4fa",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Train Epoch 1:   0%|          | 0/81 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1862242e96e6430b97d81fadaf51121d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1; train_loss=0.1418; val_loss=0.0916; val_map=0.3429\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Train Epoch 2:   0%|          | 0/81 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1385da105877471dbb08c365cedd13df"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 36\u001B[0m\n\u001B[0;32m     33\u001B[0m     total_loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m     34\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m---> 36\u001B[0m     train_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[43mtotal_loss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     38\u001B[0m     dl_train_tqdm\u001B[38;5;241m.\u001B[39mset_postfix(loss\u001B[38;5;241m=\u001B[39mtotal_loss\u001B[38;5;241m.\u001B[39mitem())\n\u001B[0;32m     40\u001B[0m train_loss \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(dl_train)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
