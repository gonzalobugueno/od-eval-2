{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-16T23:28:38.817937Z",
     "start_time": "2025-08-16T23:28:36.794115Z"
    }
   },
   "source": [
    "import albumentations.pytorch\n",
    "import customdataset as ds\n",
    "import torch as t\n",
    "import torchvision as tv\n",
    "import torchvision.tv_tensors as tvt\n",
    "import albumentations as A\n",
    "\n",
    "x, y = ds.extract_all('./datasets/090/annotations.xml')\n",
    "x1, y1 = ds.extract_all('./datasets/190/annotations.xml')\n",
    "x2, y2 = ds.extract_all('./datasets/30/annotations.xml')\n",
    "x3, y3 = ds.extract_all('./datasets/60/annotations.xml')\n",
    "x4, y4 = ds.extract_all('./datasets/90/annotations.xml')\n",
    "\n",
    "x5, y5 = ds.extract_all('./datasets/clear1/annotations.xml')\n",
    "\n",
    "train_ds = ds.CustomImageDataset([*x, *x1, *x2, *x3, *x4], [*y, *y1, *y2, *y3, *y4], transform=A.Compose([\n",
    "    A.Resize(1024, 1024),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
    "    A.ShiftScaleRotate(\n",
    "        shift_limit=0.0625,  # fraction of image height/width\n",
    "        scale_limit=0.1,  # +/- 10% scale\n",
    "        rotate_limit=15,  # degrees\n",
    "        border_mode=0,  # fill outside with 0 (black)\n",
    "        p=0.5\n",
    "    ),\n",
    "    albumentations.pytorch.ToTensorV2()\n",
    "],\n",
    "    bbox_params=ds.albumentations_params)\n",
    "                                 )\n",
    "\n",
    "val_ds = ds.CustomImageDataset(x5, y5)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "frame_000088.PNG is empty\n",
      "frame_000089.PNG is empty\n",
      "frame_000090.PNG is empty\n",
      "frame_000091.PNG is empty\n",
      "frame_000092.PNG is empty\n",
      "frame_000093.PNG is empty\n",
      "frame_000094.PNG is empty\n",
      "frame_000095.PNG is empty\n",
      "frame_000096.PNG is empty\n",
      "frame_000097.PNG is empty\n",
      "frame_000098.PNG is empty\n",
      "frame_000099.PNG is empty\n",
      "frame_000100.PNG is empty\n",
      "frame_000101.PNG is empty\n",
      "frame_000102.PNG is empty\n",
      "frame_000103.PNG is empty\n",
      "frame_000104.PNG is empty\n",
      "frame_000105.PNG is empty\n",
      "frame_000106.PNG is empty\n",
      "frame_000107.PNG is empty\n",
      "frame_000108.PNG is empty\n",
      "frame_000109.PNG is empty\n",
      "frame_000110.PNG is empty\n",
      "frame_000111.PNG is empty\n",
      "frame_000112.PNG is empty\n",
      "frame_000113.PNG is empty\n",
      "frame_000114.PNG is empty\n",
      "frame_000115.PNG is empty\n",
      "frame_000116.PNG is empty\n",
      "frame_000117.PNG is empty\n",
      "frame_000118.PNG is empty\n",
      "frame_000119.PNG is empty\n",
      "frame_000120.PNG is empty\n",
      "frame_000121.PNG is empty\n",
      "frame_000122.PNG is empty\n",
      "frame_000123.PNG is empty\n",
      "frame_000124.PNG is empty\n",
      "frame_000125.PNG is empty\n",
      "frame_000126.PNG is empty\n",
      "frame_000127.PNG is empty\n",
      "frame_000128.PNG is empty\n",
      "frame_000129.PNG is empty\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "frame_000095.PNG is empty\n",
      "frame_000096.PNG is empty\n",
      "frame_000097.PNG is empty\n",
      "frame_000098.PNG is empty\n",
      "frame_000099.PNG is empty\n",
      "frame_000100.PNG is empty\n",
      "frame_000101.PNG is empty\n",
      "frame_000102.PNG is empty\n",
      "frame_000103.PNG is empty\n",
      "frame_000104.PNG is empty\n",
      "frame_000105.PNG is empty\n",
      "frame_000106.PNG is empty\n",
      "frame_000107.PNG is empty\n",
      "frame_000108.PNG is empty\n",
      "frame_000109.PNG is empty\n",
      "frame_000110.PNG is empty\n",
      "frame_000111.PNG is empty\n",
      "frame_000112.PNG is empty\n",
      "frame_000113.PNG is empty\n",
      "frame_000114.PNG is empty\n",
      "frame_000115.PNG is empty\n",
      "frame_000116.PNG is empty\n",
      "frame_000117.PNG is empty\n",
      "frame_000118.PNG is empty\n",
      "frame_000119.PNG is empty\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "frame_000088.PNG is empty\n",
      "frame_000089.PNG is empty\n",
      "frame_000090.PNG is empty\n",
      "frame_000091.PNG is empty\n",
      "frame_000092.PNG is empty\n",
      "frame_000093.PNG is empty\n",
      "frame_000094.PNG is empty\n",
      "frame_000095.PNG is empty\n",
      "frame_000096.PNG is empty\n",
      "frame_000097.PNG is empty\n",
      "frame_000098.PNG is empty\n",
      "frame_000099.PNG is empty\n",
      "frame_000100.PNG is empty\n",
      "frame_000101.PNG is empty\n",
      "frame_000102.PNG is empty\n",
      "frame_000103.PNG is empty\n",
      "frame_000104.PNG is empty\n",
      "frame_000105.PNG is empty\n",
      "frame_000106.PNG is empty\n",
      "frame_000107.PNG is empty\n",
      "frame_000108.PNG is empty\n",
      "frame_000109.PNG is empty\n",
      "frame_000110.PNG is empty\n",
      "frame_000111.PNG is empty\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\n3\\Desktop\\od-eval-2\\.venv\\lib\\site-packages\\albumentations\\core\\validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T23:02:38.529164Z",
     "start_time": "2025-08-16T23:02:38.108224Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "device = t.device('cuda') if t.cuda.is_available() else t.device('cpu')\n",
    "\n",
    "dl_train = t.utils.data.DataLoader(train_ds, batch_size=4, shuffle=True, collate_fn=lambda e: tuple(zip(*e)))\n",
    "dl_val = t.utils.data.DataLoader(val_ds, batch_size=4, shuffle=True, collate_fn=lambda e: tuple(zip(*e)))\n",
    "\n",
    "model = tv.models.detection.fasterrcnn_resnet50_fpn(weights='COCO_V1', backbone_weights='IMAGENET1K_V2').to(device)\n",
    "\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = tv.models.detection.faster_rcnn.FastRCNNPredictor(\n",
    "    in_channels=model.roi_heads.box_predictor.cls_score.in_features,\n",
    "    num_classes=2\n",
    ").to(device)\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = t.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "lr_scheduler = t.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n"
   ],
   "id": "22669c8dbcfd5dc5",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T23:07:53.020709Z",
     "start_time": "2025-08-16T23:02:43.864360Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm.notebook import tqdm, trange\n",
    "import torchmetrics as tm\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_val_map = 0\n",
    "\n",
    "patience = 5  # stop if val_loss doesn't improve for 5 epochs\n",
    "counter = 0\n",
    "best_model_state = None\n",
    "num_epochs = 100\n",
    "metric = tm.detection.mean_ap.MeanAveragePrecision(\"xyxy\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # ---- TRAIN ----\n",
    "    model.train()\n",
    "    train_loss_epoch = 0\n",
    "\n",
    "    dl_train_tqdm = tqdm(dl_train, desc=f\"Train Epoch {epoch + 1}\", leave=False)\n",
    "\n",
    "    for images, targets in dl_train_tqdm:\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        total_loss = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss_epoch += total_loss.item()\n",
    "\n",
    "        dl_train_tqdm.set_postfix(loss=total_loss.item())\n",
    "\n",
    "    train_loss_epoch /= len(dl_train)\n",
    "\n",
    "    # ---- VALIDATION ----\n",
    "    model.eval()\n",
    "    val_loss_epoch = 0\n",
    "    metric.reset()\n",
    "\n",
    "    with t.no_grad():\n",
    "\n",
    "        dl_val_tqdm = tqdm(dl_val, desc=f\"Val Epoch {epoch + 1}\", leave=False)\n",
    "\n",
    "        for images, targets in dl_val_tqdm:\n",
    "            images = [img.to(device) for img in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            pred = model(images)\n",
    "\n",
    "            metric.update(pred, targets)\n",
    "\n",
    "    val_metrics = metric.compute()\n",
    "    val_map = val_metrics[\"map\"].item()\n",
    "    print(f\"Epoch {epoch + 1} | Train Loss: {train_loss_epoch:.4f} | Val mAP: {val_map:.4f}\")\n",
    "\n",
    "    # ---- Early Stopping ----\n",
    "    if val_map > best_val_map:\n",
    "        best_val_map = val_map\n",
    "        best_model_state = model.state_dict()\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            model.load_state_dict(best_model_state)\n",
    "            break\n"
   ],
   "id": "a83e29ab00cfc4fa",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Train Epoch 1:   0%|                                                         | 0/81 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dd9688ab548c4a02a1b63ab9d826cae7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\n3\\Desktop\\od-eval-2\\customdataset.py:122: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  areas = t.tensor(areas, dtype=t.float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Val Epoch 1:   0%|                                                            | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "434e103296c149bf9e2cd0f6e39393e9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 0.2295 | Val mAP: 0.3997\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Train Epoch 2:   0%|                                                         | 0/81 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d0b0856307464029aff843e6481896b8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Val Epoch 2:   0%|                                                            | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8e284bd131dd4165bbfe40ece593662a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Train Loss: 0.0958 | Val mAP: 0.3584\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Train Epoch 3:   0%|                                                         | 0/81 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "820e41e094694ba3922abd6156f13b14"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Val Epoch 3:   0%|                                                            | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "410888329a1a48cd93880373a04f902a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Train Loss: 0.0769 | Val mAP: 0.4129\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Train Epoch 4:   0%|                                                         | 0/81 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c78b90dbc1c84b0c9549c081637a9514"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Val Epoch 4:   0%|                                                            | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e91f0a5839a34f8eb9adf3c38a460724"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Train Loss: 0.0670 | Val mAP: 0.4252\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Train Epoch 5:   0%|                                                         | 0/81 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "41e8a8e10f3d43fc84eb90d41ac7771e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Val Epoch 5:   0%|                                                            | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "80ce49e053324361b02f793629f275e9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Train Loss: 0.0652 | Val mAP: 0.3946\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Train Epoch 6:   0%|                                                         | 0/81 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "97d642bf41b24ea28b301c74bb76c481"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AssertionError",
     "evalue": "Expected target boxes to be a tensor of shape [N, 4], got torch.Size([0]).",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 24\u001B[0m\n\u001B[0;32m     21\u001B[0m images \u001B[38;5;241m=\u001B[39m [img\u001B[38;5;241m.\u001B[39mto(device) \u001B[38;5;28;01mfor\u001B[39;00m img \u001B[38;5;129;01min\u001B[39;00m images]\n\u001B[0;32m     22\u001B[0m targets \u001B[38;5;241m=\u001B[39m [{k: v\u001B[38;5;241m.\u001B[39mto(device) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m t\u001B[38;5;241m.\u001B[39mitems()} \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m targets]\n\u001B[1;32m---> 24\u001B[0m loss_dict \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtargets\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     25\u001B[0m total_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msum\u001B[39m(loss \u001B[38;5;28;01mfor\u001B[39;00m loss \u001B[38;5;129;01min\u001B[39;00m loss_dict\u001B[38;5;241m.\u001B[39mvalues())\n\u001B[0;32m     27\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n",
      "File \u001B[1;32m~\\Desktop\\od-eval-2\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1771\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1772\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1773\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\Desktop\\od-eval-2\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1779\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1780\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1781\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1782\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1783\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1784\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1786\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1787\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\Desktop\\od-eval-2\\.venv\\lib\\site-packages\\torchvision\\models\\detection\\generalized_rcnn.py:77\u001B[0m, in \u001B[0;36mGeneralizedRCNN.forward\u001B[1;34m(self, images, targets)\u001B[0m\n\u001B[0;32m     75\u001B[0m boxes \u001B[38;5;241m=\u001B[39m target[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mboxes\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m     76\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(boxes, torch\u001B[38;5;241m.\u001B[39mTensor):\n\u001B[1;32m---> 77\u001B[0m     \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_assert\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     78\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mboxes\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mand\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mboxes\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     79\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mExpected target boxes to be a tensor of shape [N, 4], got \u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mboxes\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m.\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     80\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     81\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     82\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_assert(\n\u001B[0;32m     83\u001B[0m         \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m     84\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected target boxes to be of type Tensor, got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(boxes)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     85\u001B[0m     )\n",
      "File \u001B[1;32m~\\Desktop\\od-eval-2\\.venv\\lib\\site-packages\\torch\\__init__.py:2174\u001B[0m, in \u001B[0;36m_assert\u001B[1;34m(condition, message)\u001B[0m\n\u001B[0;32m   2168\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(condition) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mTensor \u001B[38;5;129;01mand\u001B[39;00m overrides\u001B[38;5;241m.\u001B[39mhas_torch_function(\n\u001B[0;32m   2169\u001B[0m     (condition,)\n\u001B[0;32m   2170\u001B[0m ):\n\u001B[0;32m   2171\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m overrides\u001B[38;5;241m.\u001B[39mhandle_torch_function(\n\u001B[0;32m   2172\u001B[0m         _assert, (condition,), condition, message\n\u001B[0;32m   2173\u001B[0m     )\n\u001B[1;32m-> 2174\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m condition, message\n",
      "\u001B[1;31mAssertionError\u001B[0m: Expected target boxes to be a tensor of shape [N, 4], got torch.Size([0])."
     ]
    }
   ],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
